{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](datasyn1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "The max Polling layer is the layer responsible for reducing the sensitivity to translationial variations\n",
    "in the input\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "you should use a padding of 2 on each side\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "The image dimentions is reduced from from 512 to 504. when no padding is used the dimentions of the kernel equals number of lost pixels + 1 which is 9.\n",
    "The kernel is 9x9.\n",
    "## task 1e)\n",
    "\n",
    "With the given spesifications the spatial dimensions of the pooled feature maps in the first pooling is 252x252\n",
    "## task 1f)\n",
    "\n",
    "since there is no padding and the kernel size is 3 the image will be reduced by 2 pixels in each dimention. this results in the size of the feature maps in the second layer being 250x250.\n",
    "## task 1g)\n",
    "\n",
    "number of parameters in the first layer: (5*5*3 + 1)* 32 = 2432,\n",
    "number of parameters in the second layer: (5*5*32 + 1)* 64 = 51264,\n",
    "number of parameters in the third layer: (5*5*3 + 1)* 32 = 204928,\n",
    "number of parameters in the forth layer: (5*5*3 + 1)* 32 = 131136,\n",
    "number of parameters in the fift layer: (5*5*3 + 1)* 32 = 650,\n",
    "total number of parameters: 390410\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "![](plots/task2_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "\n",
    "The final accuracy values: \n",
    "Train accuracy: 0.8770,\n",
    "Validation accuracy: 0.7296,\n",
    "Test accuracy: 0.7295"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Our first CNN had the following architecture:\n",
    "![](gulleikCNN.png)\n",
    "\n",
    "The all convolutional layers has a filter size of 3x3 with a padding of 1, and a stride of 1. The\n",
    "flatten layer takes an image with shape (Height) × (Width) × (Number of Feature Maps), and flattens\n",
    "it to a single vector with size (Height) · (Width) · (Number of Feature Maps)  Each MaxPool2D layer has a stride of 2 and a kernel size of 2 × 2. In addition to changing the filter size and the number of hidden units was regularization added in form of weight_decay in the optimizer. The level of weight decay was set to 1e-7\n",
    "\n",
    "Our second CNN had the following architecture:\n",
    "![](FredrikCNN.png)\n",
    "\n",
    "In this CNN all convolutional layers has a filter size of 5x5 with a padding of 2, and a stride of 1. The\n",
    "flatten layer takes an image with shape (Height) × (Width) × (Number of Feature Maps), and flattens\n",
    "it to a single vector with size (Height) · (Width) · (Number of Feature Maps)  Each MaxPool2D layer has a stride of 2 and a kernel size of 2 × 2.\n",
    "In contrast to the examplemodel does this model use a Dropout layer as regularization after every MaxPool2D layer, with a p value of 0.1 for the first one, and a p value of 0.05 for the second one. The ReLU activation function was changed to LeakyReLU, and BatchNorm2d was added as a batch normalization after every activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "![](task3b.PNG)\n",
    "\n",
    "Here our best model turned out to be the second one. This models results are plottet below\n",
    "\n",
    "![](plots/task3f_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "In model 1 we reduced the kernel size on the last to convolution layers, as well as reducing the padding to match this. We doubled the amount of hidden units, which increased the run time, but also increased accuracy. Last we added weight decay, a regularization technique that adds a small penalty and increased our accuracy.\n",
    "\n",
    "By changing the activation function in model 2 with LeakyReLU, we got a higher accuracy. This is due to the activation function allowing a small gradient when the unit is not active. By further implementing a BatchNorm2d we get learnable parameters which also increases the accuracy. Last we implemented a Dropout function that arbitrarily sets some elements of the input tensor to zero, with probability p. If p was high, the accuracy fell, but by decreasing p to less than 0.1, the accuracy increased. This is due to too many neurons being neutralized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3d)\n",
    "We saw the largest amount of improvement when we added the BatchNorm2d to get learnable parameters. The first graph below is before we implemented BatchNorm2d and the second graph is after.\n",
    "\n",
    "![](BeforeBatchNorm.PNG) ![](AfterBatchNorm.PNG)\n",
    "\n",
    "Here the first image is before we apply BatchNorm2d, and the second image is after we apply BatchNorm2d.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3e)\n",
    "![](plots/task3flflfkfmfd_plot.png)\n",
    "\n",
    "Our final test accuracy ended up being 0.8050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3f)\n",
    "From what we can see in the cross entropy loss plot, the validation loss saturates while the training loss continues to decrease. This is a sign of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "Briefly report your hyperparameters, including\n",
    "optimizer, batch size, learning rate and potential data augmentation used\n",
    "\n",
    "Here you can see the results from our Resnet18. For this resnet Adam was used as an optimizer, the batch size was 32, the learning rate was 5e-4, the early stop count was 4, and the weight decay was set to 0. \n",
    "While preprocessing the images, we resized them to 224x224 pixels, and we changed the mean and std to mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] for the preprocessing\n",
    "\n",
    "![](plots/task4a_plot.png)\n",
    "\n",
    "Our final test accuracy was 0.8923 or 89.23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "<img src=\"images/weights14.png\" alt=\"Drawing\"  align=\"left\" style=\"width: 138px;\">\n",
    "<img src=\"images/weights26.png\" alt=\"Drawing\"  align=\"left\" style=\"width: 138px;\">\n",
    "<img src=\"images/weights32.png\" alt=\"Drawing\"  align=\"left\" style=\"width: 138px;\">\n",
    "<img src=\"images/weights49.png\" alt=\"Drawing\"  align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/weights52.png\" alt=\"Drawing\"  align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations14.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations26.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations32.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations49.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations52.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Here we can see the different visualizations of the activations from the first convolutional layer from the fliter at index 14, 26, 32, 49, and 52.\n",
    "From what we can see is the CNN able to detect the outline of the zebra as well as its stripy pattern. Some  of the filters are also clearly able to seperate the sky form the ground in the background, while in others the background sort of blends in with the zebra. It also seems like the first filter detects vertical lines in the image while the second decects horisontal lines. It is however difficult to see what the filters do based on their visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "<img src=\"images/activations4c0.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c1.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c2.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c3.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c4.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "\n",
    "<img src=\"images/activations4c5.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/> \n",
    "<img src=\"images/activations4c6.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c7.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c8.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "<img src=\"images/activations4c9.png\" alt=\"Drawing\" align=\"left\" style=\"width: 138px;\"/>\n",
    "\n",
    "Here we can see the filter activations from running the image throu the first 10 filters of the network. We can see the the orignial image have been reduced by the polling layers to such an extent that the zebra is no longer reconizable. The filters do however return a set fo different images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
