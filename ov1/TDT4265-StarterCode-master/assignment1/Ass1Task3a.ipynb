{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "from task3a import cross_entropy_loss, SoftmaxModel, one_hot_encode\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(targets: np.ndarray, outputs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: labels/targets of each image of shape: [batch size, num_classes]\n",
    "        outputs: outputs of model of shape: [batch size, num_classes]\n",
    "    Returns:\n",
    "        Cross entropy error (float)\n",
    "    \"\"\"\n",
    "    # TODO implement this function (Task 3a)\n",
    "    assert targets.shape == outputs.shape,\\\n",
    "        f\"Targets shape: {targets.shape}, outputs: {outputs.shape}\"\n",
    "    Cn = -np.sum(targets * np.log(outputs),axis=1)\n",
    "    C = 1/(targets.shape[0]*10)*np.sum(Cn)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_images(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 784] in the range (0, 255)\n",
    "    Returns:\n",
    "        X: images of shape [batch size, 785] in the range (-1, 1)\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == 784,\\\n",
    "        f\"X.shape[1]: {X.shape[1]}, should be 784\"\n",
    "       \n",
    "    X = X - 127.5\n",
    "    X = np.divide(X, 127.5)\n",
    "    X = np.insert(X, X.shape[1], 1, axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel:\n",
    "\n",
    "    def __init__(self, l2_reg_lambda: float):\n",
    "        # Define number of input nodes\n",
    "        self.I = 785\n",
    "\n",
    "        # Define number of output nodes\n",
    "        self.num_outputs = 10\n",
    "        self.w = np.zeros((self.I, self.num_outputs))\n",
    "        self.grad = None\n",
    "\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "        Returns:\n",
    "            y: output of model with shape [batch size, num_outputs]\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO implement this function (Task 3a)\n",
    "        \n",
    "        z_k = np.exp(np.dot(X,self.w))\n",
    "        y_k = z_k/np.sum(z_k, axis = 1, keepdims = True)\n",
    "        \n",
    "        return y_k\n",
    "\n",
    "    def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Computes the gradient and saves it to the variable self.grad\n",
    "\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "            outputs: outputs of model of shape: [batch size, num_outputs]\n",
    "            targets: labels/targets of each image of shape: [batch size, num_classes]\n",
    "        \"\"\"\n",
    "        # TODO implement this function (Task 3a)\n",
    "        # To implement L2 regularization task (4b) you can get the lambda value in self.l2_reg_lambda \n",
    "        # which is defined in the constructor.\n",
    "        assert targets.shape == outputs.shape,\\\n",
    "            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n",
    "        self.grad = np.zeros_like(self.w)\n",
    "        assert self.grad.shape == self.w.shape,\\\n",
    "             f\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\n",
    "        self.grad = np.dot(-X.T,targets-outputs)/(self.num_outputs*X.shape[0]) \n",
    "        \"+ self.l2_reg_lambda*self.w (4b)\"\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y: np.ndarray, num_classes: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Y: shape [Num examples, 1]\n",
    "        num_classes: Number of classes to use for one-hot encoding\n",
    "    Returns:\n",
    "        Y: shape [Num examples, num classes]\n",
    "    \"\"\"\n",
    "    \n",
    "    encode = np.zeros([Y.shape[0], num_classes])\n",
    "    for i in range (Y.shape[0]):\n",
    "        encode[i,Y[i]] = 1\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_approximation_test(model: SoftmaxModel, X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "        Numerical approximation for gradients. Should not be edited. \n",
    "        Details about this test is given in the appendix in the assignment.\n",
    "    \"\"\"\n",
    "    w_orig = np.random.normal(loc=0, scale=1/model.w.shape[0]**2, size=model.w.shape)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    for i in range(model.w.shape[0]):\n",
    "        for j in range(model.w.shape[1]):\n",
    "            model.w = w_orig.copy()\n",
    "            orig = model.w[i, j].copy()\n",
    "            model.w[i, j] = orig + epsilon\n",
    "            logits = model.forward(X)\n",
    "            cost1 = cross_entropy_loss(Y, logits)\n",
    "            model.w[i, j] = orig - epsilon\n",
    "            logits = model.forward(X)\n",
    "            cost2 = cross_entropy_loss(Y, logits)\n",
    "            gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n",
    "            model.w[i, j] = orig\n",
    "            # Actual gradient\n",
    "            logits = model.forward(X)\n",
    "            model.backward(X, logits, Y)\n",
    "            difference = gradient_approximation - model.grad[i, j]\n",
    "            assert abs(difference) <= epsilon**2,\\\n",
    "                f\"Calculated gradient is incorrect. \" \\\n",
    "                f\"Approximation: {gradient_approximation}, actual gradient: {model.grad[i, j]}\\n\" \\\n",
    "                f\"If this test fails there could be errors in your cross entropy loss function, \" \\\n",
    "                f\"forward function or backward function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: X: (20000, 784), Y: (20000, 1)\n",
      "Validation shape: X: (10000, 784), Y: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Simple test on one-hot encoding\n",
    "    Y = np.zeros((1, 1), dtype=int)\n",
    "    Y[0, 0] = 3\n",
    "    Y = one_hot_encode(Y, 10)\n",
    "    assert Y[0, 3] == 1 and Y.sum() == 1, \\\n",
    "        f\"Expected the vector to be [0,0,0,1,0,0,0,0,0,0], but got {Y}\"\n",
    "\n",
    "    X_train, Y_train, *_ = utils.load_full_mnist()\n",
    "    X_train = pre_process_images(X_train)\n",
    "    Y_train = one_hot_encode(Y_train, 10)\n",
    "    assert X_train.shape[1] == 785,\\\n",
    "        f\"Expected X_train to have 785 elements per image. Shape was: {X_train.shape}\"\n",
    "\n",
    "    # Simple test for forward pass. Note that this does not cover all errors!\n",
    "    model = SoftmaxModel(0.0)\n",
    "    logits = model.forward(X_train)\n",
    "    np.testing.assert_almost_equal(\n",
    "        logits.mean(), 1/10,\n",
    "        err_msg=\"Since the weights are all 0's, the softmax activation should be 1/10\")\n",
    "\n",
    "    # Gradient approximation check for 100 images\n",
    "    X_train = X_train[:100]\n",
    "    Y_train = Y_train[:100]\n",
    "    for i in range(2):\n",
    "        gradient_approximation_test(model, X_train, Y_train)\n",
    "        model.w = np.random.randn(*model.w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
