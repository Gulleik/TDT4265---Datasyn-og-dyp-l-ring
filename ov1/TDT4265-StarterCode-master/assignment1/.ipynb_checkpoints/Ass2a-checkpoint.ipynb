{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_images(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 784] in the range (0, 255)\n",
    "    Returns:\n",
    "        X: images of shape [batch size, 785] in the range (-1, 1)\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == 784,\\\n",
    "        f\"X.shape[1]: {X.shape[1]}, should be 784\"\n",
    "    normalizer = lambda t: ((2*t)/255) -1\n",
    "    normalized_array = np.zeros((len(X), 785), np.float16)\n",
    "    for index, elem in enumerate(X):\n",
    "        normalized_array[index] = np.insert(np.array([normalizer(i) for i in elem]), 0, 1, axis=0)\n",
    "    return normalized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(targets: np.ndarray, outputs: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        outputs: outputs of model of shape: [batch size, 1]\n",
    "    Returns:\n",
    "        Cross entropy error (float)\n",
    "    \"\"\"\n",
    "    # TODO implement this function (Task 2a)\n",
    "    assert targets.shape == outputs.shape,\\\n",
    "        f\"Targets shape: {targets.shape}, outputs: {outputs.shape}\"\n",
    "    \n",
    "    batch_size = len(targets)\n",
    "    ln = lambda t: np.ln(t)\n",
    "    Cn  = -(targets*np.log(outputs) + (1 - targets)*np.log(1-outputs))\n",
    "    Entropy_error = np.sum(a=Cn,axis=0)/batch_size\n",
    "    return Entropy_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define number of input nodes\n",
    "        self.I = 785\n",
    "        self.w = np.zeros((self.I, 1))\n",
    "        self.grad = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "        Returns:\n",
    "            y: output of model with shape [batch size, 1]\n",
    "        \"\"\"\n",
    "        y = sigmoid(np.dot(X,self.w))\n",
    "        return y\n",
    "\n",
    "    def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Computes the gradient and saves it to the variable self.grad\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "            outputs: outputs of model of shape: [batch size, 1]\n",
    "            targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        \"\"\"\n",
    "        # TODO implement this function (Task 2a)\n",
    "        assert targets.shape == outputs.shape,\\\n",
    "            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n",
    "        self.grad = np.zeros_like(self.w)\n",
    "        assert self.grad.shape == self.w.shape,\\\n",
    "            f\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\n",
    "        delta = targets - outputs\n",
    "        self.grad = np.dot(delta,-X)\n",
    "        temp_self_grad = self.grad\n",
    "        self.grad = temp_self_grad/X.shape[0]\n",
    "        \n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_approximation_test(model: BinaryModel, X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "        Numerical approximation for gradients. Should not be edited. \n",
    "        Details about this test is given in the appendix in the assignment.\n",
    "    \"\"\"\n",
    "    w_orig = np.random.normal(loc=0, scale=1/model.w.shape[0]**2, size=model.w.shape)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w_orig.shape[0]):\n",
    "        model.w = w_orig.copy()\n",
    "        orig = w_orig[i].copy()\n",
    "        model.w[i] = orig + epsilon\n",
    "        logits = model.forward(X)\n",
    "        cost1 = cross_entropy_loss(Y, logits)\n",
    "        model.w[i] = orig - epsilon\n",
    "        logits = model.forward(X)\n",
    "        cost2 = cross_entropy_loss(Y, logits)\n",
    "        gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n",
    "        model.w[i] = orig\n",
    "        # Actual gradient\n",
    "        logits = model.forward(X)\n",
    "        model.backward(X, logits, Y)\n",
    "        difference = gradient_approximation - model.grad[i, 0]\n",
    "        assert abs(difference) <= epsilon**2,\\\n",
    "            f\"Calculated gradient is incorrect. \" \\\n",
    "            f\"Approximation: {gradient_approximation}, actual gradient: {model.grad[i,0]}\\n\" \\\n",
    "            f\"If this test fails there could be errors in your cross entropy loss function, \" \\\n",
    "            f\"forward function or backward function\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: X: (4005, 784), Y: (4005, 1)\n",
      "Validation shape: X: (2042, 784), Y: (2042, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,1) and (100,785) not aligned: 1 (dim 1) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-1240671b2faf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mgradient_approximation_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-1255152f74ec>\u001b[0m in \u001b[0;36mgradient_approximation_test\u001b[1;34m(model, X, Y)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Actual gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mdifference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_approximation\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-9421d74b6002>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X, outputs, targets)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;34mf\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mtemp_self_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_self_grad\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,1) and (100,785) not aligned: 1 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    category1, category2 = 2, 3\n",
    "    X_train, Y_train, *_ = utils.load_binary_dataset(category1, category2)\n",
    "    X_train = pre_process_images(X_train)\n",
    "    assert X_train.max() <= 1.0, f\"The images (X_train) should be normalized to the range [-1, 1]\"\n",
    "    assert X_train.min() < 0 and X_train.min() >= -1, f\"The images (X_train) should be normalized to the range [-1, 1]\"\n",
    "    assert X_train.shape[1] == 785,\\\n",
    "        f\"Expected X_train to have 785 elements per image. Shape was: {X_train.shape}\"\n",
    "\n",
    "    # Simple test for forward pass. Note that this does not cover all errors!\n",
    "    model = BinaryModel()\n",
    "    logits = model.forward(X_train)\n",
    "    np.testing.assert_almost_equal(\n",
    "        logits.mean(), .5,\n",
    "        err_msg=\"Since the weights are all 0's, the sigmoid activation should be 0.5\")\n",
    "\n",
    "    # Gradient approximation check for 100 images\n",
    "    X_train = X_train[:100]\n",
    "    Y_train = Y_train[:100]\n",
    "    for i in range(2):\n",
    "        gradient_approximation_test(model, X_train, Y_train)\n",
    "        model.w = np.random.randn(*model.w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
