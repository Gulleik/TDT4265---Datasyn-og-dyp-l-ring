{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](Øving2_page1.png)\n",
    "![](Øving2_page2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![](Øving2_page3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Number of parameters in the network, n, is equal to the number of weights, w, and the bias, b.\n",
    "\n",
    "n = w + b = 784 * 64 + 10 * 64 + 64 = 50880\n",
    "\n",
    "The number of parameters in the network is thus equal to 50880."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "3a) \n",
    "\n",
    "![](Improved_weight.png)\n",
    "\n",
    "3b)\n",
    "\n",
    "![](Improved_weight_sigmoid2.png)\n",
    "\n",
    "3c)\n",
    "\n",
    "![](Improved_weight_sigmoid_moment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial results without any additions were 0.96 for training accuracy and 0.89 for the validation accuracy. With all the additions present, we reached a training accuracy of 1 and a validation accuracy 0.96. \n",
    "\n",
    "With the addition of the improved weights our validation accuracy improved from 0.89 to 0.95, however the convergence rate increased a little bit resulting in a . The improved sigmoid improved the validation accuracy a little bit more up to 0.96, with a small increase in convergence speed resulting in an early stop at 19 epochs. The convergence speed was however greatly improved when the momentum step was introduced. Reducing our runtime considerably, and triggering an early stop at 14 epochs.\n",
    " The combination of all three additions resulted in a training accuracy of 1. This can be considered a strong sign of overfitting, especially since the validation accuracy capped at 0.96. This effect can also be seen in the cross entropy loss there the validation loss and the training loss also starts to diverge towards the end. The validation accuracy is however quite high, which means that our network is able to produce good results regardless of the overfitting signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "If the number of units is set too 32 then the run time is significantly less and the train accuracy is reduced from 1 to 0.99465. The validation accuracy is reduced from 0.9603 to 0.9496. Thus the accuracy is still good even though the number of hidden units is small. The reason for this might be because our model is too complex.\n",
    "\n",
    "![](4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "By increasing the number of hidden units to 128, the run time is almost doubled, while the train accuracy is still 1. The validation accuracy is increased from 0.9603 to 0.9705. Thus the validation accuracy is better by a small amount, but at the cost of a large increase in run time.\n",
    "\n",
    "![](4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "In the previous network we had 50880 parameters. In our new network, the number of paramteres has increased to 51300, as we now have 2 hidden layers with 60 neurons in each. \n",
    "Train accuracy reduces from 1 to 0.99995 with the new layer. The validation accuracy reduces to 0.9629 from 0.9603.\n",
    "\n",
    "![](4d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "The train accuracy decreases from 1 to 0.978, while the validation accuracy decreases from 0.9629 to 0.945.\n",
    "The gradient deminishes with each layer, thus the weights in the early layers will barely be updated. Backward propegating does not work well in this case resulting in a network with randomized weights for the early layers. The last layers thus has to compensate as of this huge architecture.\n",
    "\n",
    "The task asks us to: \"Plot the training loss in the same graph as your baseline from task 3\". We assume this means that we should plot the graphs with the same axis and template as the graphs in task 3. \n",
    "\n",
    "![](4e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
