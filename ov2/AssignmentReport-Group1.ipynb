{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](Øving2_page1.png)\n",
    "![](Øving2_page2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![](Øving2_page3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Number of parameters in the network, n, is equal to the number of weights, w, and the bias, b.\n",
    "\n",
    "n = w + b = 784 * 64 + 10 * 64 + 64 = 50880\n",
    "\n",
    "The number of parameters in the network is thus equal to 50880."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "3a) The network with improved weights\n",
    "\n",
    "![](Improved_weight.png)\n",
    "\n",
    "3b) The network with improved weights and an improved sigmoid. \n",
    "\n",
    "![](Improved_weight_sigmoid2.png)\n",
    "\n",
    "3c) The network with improved weights, an improved sigmoid and momentum. \n",
    "\n",
    "![](Improved_weight_sigmoid_moment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial results without any additions were 0.96 for training accuracy and 0.89 for the validation accuracy. With all the additions present, we reached a training accuracy of 1 and a validation accuracy 0.96. \n",
    "\n",
    "With the addition of the improved weights our validation accuracy improved from 0.89 to 0.95, however the convergence rate increased a little bit resulting in a eraly stop at 46 epochs. The improved sigmoid improved the validation accuracy a little bit more up to 0.96, but the convergence rate increased significantly which resultet in an eraly stop at 19 epochs. The convergence speed was again improved when the momentum was introduced. Reducing our runtime, and triggering an early stop at 14 epochs.\n",
    "\n",
    "The combination of all three additions resulted in a training accuracy of 1. This can be considered a strong sign of overfitting, especially since the validation accuracy capped at 0.96. This effect can also be seen in the cross entropy loss there the validation loss and the training loss also starts to diverge towards the end. The validation accuracy is however quite high, which means that our network is able to produce good results (is general enough) regardless of the overfitting signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "If the number of neurons in the hidden layer is set to 32 then the run time is significantly reduced compared to using 64 neurons and the train accuracy was reduced from 1 to 0.99465. The validation accuracy was also reduced from 0.9603 to 0.9496. Thus the accuracy is still good even though the number of hidden units is smaller. This might be an indication of us using a more complex network than necessary\n",
    "\n",
    "![](4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "By increasing the number of hidden neurons to 128, the run time was almost doubled, while the train accuracy was still 1. The validation accuracy however increased from 0.9603 to 0.9705. Thus the validation accuracy is better by a small amount, but at the cost of a large increase in run time by doubling the number of hidden neurons.\n",
    "\n",
    "![](4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "In the previous network we had 50880 parameters. In our new network, the number of parameteres has increased to 51300, as we now have 2 hidden layers with 60 neurons in each. \n",
    "Train accuracy was reduced from 1 to 0.99995 with the new layer, while the validation accuracy was reduces to 0.9629 from 0.9603. This indicates that adding a new layer but keeping the number of parameters about the same has almost no effect in our current case.\n",
    "\n",
    "![](4d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "By creating a network with 10 hidden layers consisting of 64 neurons each the train accuracy decreased from 1 to 0.978, while the validation accuracy decreased from 0.9629 to 0.945.\n",
    "\n",
    "This small reduction in performance comes from the gradient deminishing with each layer. This results in the weights of the early layers barely being updated. Backwards propagation does not work well whith deep neural networks because of this, and the result ends up being that the weights in the early layers barely being updated / trained, and thus randomising our initial data. The later layers are however able to compensate for this, but not enough to avoid a decrease in accuracy.\n",
    "\n",
    "The task asks us to: \"Plot the training loss in the same graph as your baseline from task 3\". We assume this means that we should plot the graphs with the same axis and template as the graphs in task 3. \n",
    "\n",
    "![](4e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
